# 작업 이력 - 2025-01-06

## 19:30 - CloudWatch 모듈 분리 및 05-cloudwatch 레이어 생성

### 변경 사항

- 신규 파일: `modules/cloudwatch/main.tf`
  - EKS 클러스터 로그 그룹
  - VPC Flow Logs 로그 그룹 (선택적)

- 신규 파일: `modules/cloudwatch/variables.tf`
  - 프로젝트/환경/클러스터 설정
  - 로그 보존 기간 설정
  - KMS 암호화 설정

- 신규 파일: `modules/cloudwatch/outputs.tf`
  - 로그 그룹 이름 및 ARN 출력

- 신규 파일: `environments/prod/05-cloudwatch/terragrunt.hcl`
  - foundation 의존성
  - common.hcl에서 설정값 참조

- 수정 파일: `modules/eks-cluster/main.tf`
  - CloudWatch Log Group 리소스 제거 (별도 모듈로 분리)

- 수정 파일: `environments/prod/30-eks-cluster/terragrunt.hcl`
  - 05-cloudwatch 의존성 추가

- 수정 파일: `.github/workflows/terragrunt-apply.yml`
  - 05-cloudwatch 레이어 추가

- 수정 파일: `.github/workflows/terragrunt-destroy.yml`
  - 05-cloudwatch 레이어 추가
  - 삭제 순서 업데이트

### 배포 순서

```text
00-foundation → 05-cloudwatch → 10-networking → 20-security → 30-eks-cluster → ...
```

### 삭제 순서

```text
60-database → 50-addons → 40-nodegroups → 30-eks-cluster → 20-security → 10-networking → 05-cloudwatch → 00-foundation
```

### 설계 이유

- CloudWatch Log Group은 EKS 클러스터보다 먼저 생성되어야 함
- EKS가 자동으로 Log Group을 생성하면 Terraform 관리 불가
- 별도 모듈로 분리하여 독립적인 라이프사이클 관리

---

## 18:30 - 전체 인프라 Destroy 완료 및 검증

### 작업 내용

- GitHub Actions Destroy 워크플로우 실행 (all 옵션)
- 삭제 순서: 60-database → 50-addons → 40-nodegroups → 30-eks-cluster → 20-security → 10-networking → 00-foundation

### 삭제 확인 결과

| 리소스 | 상태 |
|--------|------|
| EKS Cluster (jsj-eks-cluster) | ✅ 삭제됨 |
| Aurora MySQL (jsj-eks-aurora-mysql) | ✅ 삭제됨 |
| VPC (jsj-eks-vpc) | ✅ 삭제됨 |
| S3 State Bucket | ✅ 삭제됨 |
| DynamoDB Lock Table | ✅ 삭제됨 |
| NAT Gateway | ✅ 삭제됨 |
| Security Groups | ✅ 삭제됨 |
| KMS Key | ✅ 삭제됨 |
| IAM Roles | ✅ 삭제됨 |
| CloudWatch Log Group | ⚠️ 수동 삭제 (Terraform 미관리) |

### 수동 삭제 명령어

```bash
aws logs delete-log-group --log-group-name "/aws/eks/jsj-eks-cluster/cluster" --region ap-northeast-2
```

---

## 17:35 - Destroy 워크플로우 all 옵션 추가 및 보안 조치

### 변경 사항

- 수정 파일: `.github/workflows/terragrunt-destroy.yml`
  - `all` 옵션 추가 (전체 인프라 역순 삭제)
  - 삭제 순서: 60-database → 50-addons → 40-nodegroups → 30-eks-cluster → 20-security → 10-networking → 00-foundation
  - `destroy-all` job과 `destroy-single` job 분리

- 신규 파일: `k8s-manifests/petclinic/` (3-tier 아키텍처 테스트용)
  - `00-namespace.yaml`: petclinic namespace
  - `01-secrets.yaml`: Aurora 연결 정보 (placeholder 사용)
  - `02-petclinic-deployment.yaml`: mysql-client, web-app
  - `03-nginx-proxy.yaml`: Nginx reverse proxy

### 보안 조치

- `01-secrets.yaml`에 민감정보 커밋 발견
- `git commit --amend` + `git push --force`로 history 덮어쓰기
- 민감정보를 `<PLACEHOLDER>` 형식으로 교체

### 테스트 리소스 정리

- petclinic, demo-app namespace 삭제 완료
- Terragrunt destroy 준비 완료

---

## 16:10 - Aurora MySQL Terraform 모듈 및 GitHub Actions 워크플로우 구성

### 변경 사항

- 신규 파일: `modules/aurora-mysql/variables.tf`
  - Aurora 클러스터 설정 변수 정의
  - 인스턴스 클래스, 백업, 모니터링 설정

- 신규 파일: `modules/aurora-mysql/main.tf`
  - Aurora MySQL 클러스터 및 인스턴스 리소스
  - Security Group, Subnet Group, Parameter Group
  - Secrets Manager 연동 (자동 비밀번호 생성)
  - Enhanced Monitoring IAM Role

- 신규 파일: `modules/aurora-mysql/outputs.tf`
  - 클러스터 엔드포인트, Reader 엔드포인트
  - Security Group ID, Secrets Manager ARN

- 신규 파일: `modules/aurora-mysql/versions.tf`
  - Terraform, AWS Provider 버전 요구사항

- 신규 파일: `environments/prod/60-database/terragrunt.hcl`
  - Aurora 레이어 Terragrunt 설정
  - networking, security, foundation 의존성

- 수정 파일: `environments/prod/common.hcl`
  - Aurora MySQL 설정 블록 추가
  - Writer 1개 + Reader 1개 구성
  - db.t3.medium 인스턴스 (Aurora 최소 사양)

- 수정 파일: `modules/networking/outputs.tf`
  - `private_subnet_cidrs` output 추가

### Aurora 구성 상세

```hcl
aurora = {
  cluster_identifier = "jsj-eks-aurora-mysql"
  engine_version     = "8.0.mysql_aurora.3.05.2"
  database_name      = "petclinic"
  instance_class     = "db.t3.medium"

  instances = {
    writer = { promotion_tier = 0 }
    reader = { promotion_tier = 1 }
  }
}
```

### 아키텍처

```text
┌─────────────────────────────────────────────────────────┐
│                      VPC (10.0.0.0/16)                  │
│  ┌─────────────────┐    ┌─────────────────┐            │
│  │ Database Subnet │    │ Database Subnet │            │
│  │   10.0.20.0/24  │    │   10.0.21.0/24  │            │
│  │      (AZ-a)     │    │      (AZ-c)     │            │
│  │  ┌───────────┐  │    │  ┌───────────┐  │            │
│  │  │  Writer   │  │    │  │  Reader   │  │            │
│  │  │ (Primary) │◀─┼────┼──│ (Replica) │  │            │
│  │  └───────────┘  │    │  └───────────┘  │            │
│  └─────────────────┘    └─────────────────┘            │
│           ▲                                             │
│           │ MySQL 3306                                  │
│  ┌────────┴────────┐                                   │
│  │  EKS Worker     │                                   │
│  │  Nodes (Private)│                                   │
│  └─────────────────┘                                   │
└─────────────────────────────────────────────────────────┘
```

### 영향도

- 새로운 60-database 레이어 추가
- networking, security 모듈 의존성
- 배포 시 Aurora 인스턴스 2개 생성 (비용 발생)

### 배포 방법

```bash
cd environments/prod/60-database
terragrunt init
terragrunt plan
terragrunt apply
```

---

## 15:40 - EKS 클러스터 애플리케이션 배포 테스트

### 변경 사항

- 신규 파일: `k8s-manifests/demo-app/nginx-web.yaml`
  - Nginx 웹서버 Deployment (2 replicas)
  - ConfigMap을 통한 커스텀 index.html
  - LoadBalancer Service (NLB, internet-facing)

- 신규 파일: `k8s-manifests/demo-app/tomcat-was.yaml`
  - Tomcat WAS Deployment (2 replicas)
  - ClusterIP Service (내부 통신용)
  - LoadBalancer Service (외부 테스트용)

### 작업 내용

1. **kubectl EKS 연결 설정**
   - `aws eks update-kubeconfig --name jsj-eks-cluster` 실행
   - IAM 사용자 Access Entry 추가 필요 (GitHub Actions Role만 기본 등록)

2. **EKS Access Entry 추가**
   - `aws eks create-access-entry` 명령으로 IAM 사용자 등록
   - `AmazonEKSClusterAdminPolicy` 연결로 클러스터 관리자 권한 부여

3. **demo-app namespace 생성**
   - `kubectl create namespace demo-app`

4. **Nginx 웹서버 배포**
   - 이미지: `nginx:1.25-alpine`
   - 커스텀 HTML 페이지 ConfigMap으로 마운트
   - Resource limits: 100m CPU, 128Mi Memory

5. **Tomcat WAS 배포**
   - 이미지: `tomcat:10-jdk17-temurin-jammy`
   - TCP Socket probe로 변경 (기본 ROOT 앱 없음)
   - Resource limits: 500m CPU, 512Mi Memory

### 배포 결과

| 리소스 | 상태 | 접속 URL |
|--------|------|----------|
| Nginx Web | 2/2 Running | `http://a9a81639e2c834458b933d49808a578d-91a72e901c368a05.elb.ap-northeast-2.amazonaws.com` |
| Tomcat WAS | 2/2 Running | `http://a9f9620c7c1f24e8a9bc1dcebca9beda-914ef21941e70cec.elb.ap-northeast-2.amazonaws.com:8080` |

### 접속 테스트

- Nginx: HTTP 200 OK (커스텀 HTML 페이지 정상 표시)
- Tomcat: HTTP 404 (정상 - 기본 ROOT 앱 없음, 서버 동작 확인)

### 리소스 영향

- 새로운 NLB 2개 생성 (AWS 비용 발생)
- demo-app namespace에 4개 Pod 실행
- Public 인터넷에서 접근 가능 (테스트 후 삭제 권장)

### 다음 단계 (선택)

- Ingress Controller 설정으로 단일 ALB 사용
- 실제 애플리케이션 WAR 파일 배포
- HPA (Horizontal Pod Autoscaler) 설정
- 테스트 완료 후 리소스 정리: `kubectl delete namespace demo-app`

---

## 15:30 - EKS 인프라 배포 완료 확인

### 변경 사항

- AWS CLI로 배포된 EKS 인프라 검증

### 검증 결과

| 리소스 | 설정값 | 상태 |
|--------|--------|------|
| EKS Cluster | jsj-eks-cluster (v1.31) | ACTIVE |
| VPC | 10.0.0.0/16 | 생성완료 |
| Subnets | 8개 (public/private/database/pod) | 생성완료 |
| Node Groups | System(1), Application(1) | ACTIVE |
| Add-ons | 5개 (vpc-cni, coredns, kube-proxy, ebs-csi, pod-identity) | ACTIVE |

### 노드 현황

- System Node Group: 1 node (t3.small)
- Application Node Group: 1 node (t3.small)
- 총 2개 워커 노드 실행 중
