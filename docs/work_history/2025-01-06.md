# 작업 이력 - 2025-01-06

## 16:10 - Aurora MySQL Terraform 모듈 구성

### 변경 사항

- 신규 파일: `modules/aurora-mysql/variables.tf`
  - Aurora 클러스터 설정 변수 정의
  - 인스턴스 클래스, 백업, 모니터링 설정

- 신규 파일: `modules/aurora-mysql/main.tf`
  - Aurora MySQL 클러스터 및 인스턴스 리소스
  - Security Group, Subnet Group, Parameter Group
  - Secrets Manager 연동 (자동 비밀번호 생성)
  - Enhanced Monitoring IAM Role

- 신규 파일: `modules/aurora-mysql/outputs.tf`
  - 클러스터 엔드포인트, Reader 엔드포인트
  - Security Group ID, Secrets Manager ARN

- 신규 파일: `modules/aurora-mysql/versions.tf`
  - Terraform, AWS Provider 버전 요구사항

- 신규 파일: `environments/prod/60-database/terragrunt.hcl`
  - Aurora 레이어 Terragrunt 설정
  - networking, security, foundation 의존성

- 수정 파일: `environments/prod/common.hcl`
  - Aurora MySQL 설정 블록 추가
  - Writer 1개 + Reader 1개 구성
  - db.t3.medium 인스턴스 (Aurora 최소 사양)

- 수정 파일: `modules/networking/outputs.tf`
  - `private_subnet_cidrs` output 추가

### Aurora 구성 상세

```hcl
aurora = {
  cluster_identifier = "jsj-eks-aurora-mysql"
  engine_version     = "8.0.mysql_aurora.3.05.2"
  database_name      = "petclinic"
  instance_class     = "db.t3.medium"

  instances = {
    writer = { promotion_tier = 0 }
    reader = { promotion_tier = 1 }
  }
}
```

### 아키텍처

```text
┌─────────────────────────────────────────────────────────┐
│                      VPC (10.0.0.0/16)                  │
│  ┌─────────────────┐    ┌─────────────────┐            │
│  │ Database Subnet │    │ Database Subnet │            │
│  │   10.0.20.0/24  │    │   10.0.21.0/24  │            │
│  │      (AZ-a)     │    │      (AZ-c)     │            │
│  │  ┌───────────┐  │    │  ┌───────────┐  │            │
│  │  │  Writer   │  │    │  │  Reader   │  │            │
│  │  │ (Primary) │◀─┼────┼──│ (Replica) │  │            │
│  │  └───────────┘  │    │  └───────────┘  │            │
│  └─────────────────┘    └─────────────────┘            │
│           ▲                                             │
│           │ MySQL 3306                                  │
│  ┌────────┴────────┐                                   │
│  │  EKS Worker     │                                   │
│  │  Nodes (Private)│                                   │
│  └─────────────────┘                                   │
└─────────────────────────────────────────────────────────┘
```

### 영향도

- 새로운 60-database 레이어 추가
- networking, security 모듈 의존성
- 배포 시 Aurora 인스턴스 2개 생성 (비용 발생)

### 배포 방법

```bash
cd environments/prod/60-database
terragrunt init
terragrunt plan
terragrunt apply
```

---

## 15:40 - EKS 클러스터 애플리케이션 배포 테스트

### 변경 사항

- 신규 파일: `k8s-manifests/demo-app/nginx-web.yaml`
  - Nginx 웹서버 Deployment (2 replicas)
  - ConfigMap을 통한 커스텀 index.html
  - LoadBalancer Service (NLB, internet-facing)

- 신규 파일: `k8s-manifests/demo-app/tomcat-was.yaml`
  - Tomcat WAS Deployment (2 replicas)
  - ClusterIP Service (내부 통신용)
  - LoadBalancer Service (외부 테스트용)

### 작업 내용

1. **kubectl EKS 연결 설정**
   - `aws eks update-kubeconfig --name jsj-eks-cluster` 실행
   - IAM 사용자 Access Entry 추가 필요 (GitHub Actions Role만 기본 등록)

2. **EKS Access Entry 추가**
   - `aws eks create-access-entry` 명령으로 IAM 사용자 등록
   - `AmazonEKSClusterAdminPolicy` 연결로 클러스터 관리자 권한 부여

3. **demo-app namespace 생성**
   - `kubectl create namespace demo-app`

4. **Nginx 웹서버 배포**
   - 이미지: `nginx:1.25-alpine`
   - 커스텀 HTML 페이지 ConfigMap으로 마운트
   - Resource limits: 100m CPU, 128Mi Memory

5. **Tomcat WAS 배포**
   - 이미지: `tomcat:10-jdk17-temurin-jammy`
   - TCP Socket probe로 변경 (기본 ROOT 앱 없음)
   - Resource limits: 500m CPU, 512Mi Memory

### 배포 결과

| 리소스 | 상태 | 접속 URL |
|--------|------|----------|
| Nginx Web | 2/2 Running | `http://a9a81639e2c834458b933d49808a578d-91a72e901c368a05.elb.ap-northeast-2.amazonaws.com` |
| Tomcat WAS | 2/2 Running | `http://a9f9620c7c1f24e8a9bc1dcebca9beda-914ef21941e70cec.elb.ap-northeast-2.amazonaws.com:8080` |

### 접속 테스트

- Nginx: HTTP 200 OK (커스텀 HTML 페이지 정상 표시)
- Tomcat: HTTP 404 (정상 - 기본 ROOT 앱 없음, 서버 동작 확인)

### 리소스 영향

- 새로운 NLB 2개 생성 (AWS 비용 발생)
- demo-app namespace에 4개 Pod 실행
- Public 인터넷에서 접근 가능 (테스트 후 삭제 권장)

### 다음 단계 (선택)

- Ingress Controller 설정으로 단일 ALB 사용
- 실제 애플리케이션 WAR 파일 배포
- HPA (Horizontal Pod Autoscaler) 설정
- 테스트 완료 후 리소스 정리: `kubectl delete namespace demo-app`

---

## 15:30 - EKS 인프라 배포 완료 확인

### 변경 사항

- AWS CLI로 배포된 EKS 인프라 검증

### 검증 결과

| 리소스 | 설정값 | 상태 |
|--------|--------|------|
| EKS Cluster | jsj-eks-cluster (v1.31) | ACTIVE |
| VPC | 10.0.0.0/16 | 생성완료 |
| Subnets | 8개 (public/private/database/pod) | 생성완료 |
| Node Groups | System(1), Application(1) | ACTIVE |
| Add-ons | 5개 (vpc-cni, coredns, kube-proxy, ebs-csi, pod-identity) | ACTIVE |

### 노드 현황

- System Node Group: 1 node (t3.small)
- Application Node Group: 1 node (t3.small)
- 총 2개 워커 노드 실행 중
